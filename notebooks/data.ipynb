{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import lru_cache, partial\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import albumentations as albu\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import ConcatDataset as ConcatDataset\n",
    "\n",
    "from shallow import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, root, pattern):\n",
    "        self.root = Path(root)\n",
    "        self.pattern = pattern\n",
    "        self.files = sorted(list(self.root.glob(self.pattern)))\n",
    "        self._is_empty('There is no matching files!')\n",
    "        \n",
    "    def apply_filter(self, filter_fn):\n",
    "        self.files = filter_fn(self.files)\n",
    "        self._is_empty()\n",
    "\n",
    "    def _is_empty(self, msg='There is no item in dataset!'): assert len(self.files) > 0\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, idx): return self.process_item(self.load_item(idx))\n",
    "    def load_item(self, idx): raise NotImplementedError\n",
    "    def process_item(self, item): return item\n",
    "#     def __add__(self, other):\n",
    "#         return ConcatDataset([self, other])\n",
    "    \n",
    "class ImageDataset(Dataset):\n",
    "    def load_item(self, idx):\n",
    "        img_path = self.files[idx]\n",
    "        img = Image.open(str(img_path))\n",
    "        return img\n",
    "    \n",
    "class PairDataset:\n",
    "    def __init__(self, ds1, ds2):\n",
    "        self.ds1, self.ds2 = ds1, ds2\n",
    "        self.check_len()\n",
    "    \n",
    "    def __len__(self): return len(self.ds1)\n",
    "    def check_len(self): assert len(self.ds1) == len(self.ds2)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ds1.__getitem__(idx), self.ds2.__getitem__(idx) \n",
    "    \n",
    "    \n",
    "class TransformDataset:\n",
    "    def __init__(self, dataset, transforms, is_masked=False):\n",
    "        self.dataset = dataset\n",
    "        self.transforms = albu.Compose([]) if transforms is None else transforms\n",
    "        self.is_masked = is_masked\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset.__getitem__(idx)\n",
    "        if self.is_masked:\n",
    "            img, mask = item\n",
    "            augmented = self.transforms(image=img, mask=mask)\n",
    "            return augmented[\"image\"], augmented[\"mask\"]\n",
    "        else:\n",
    "            return self.transforms(image=item[0], mask=None)['image']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "class MultiplyDataset:\n",
    "    def __init__(self, dataset, rate):\n",
    "        _dataset = ConcatDataset([dataset])\n",
    "        for i in range(rate-1):\n",
    "            _dataset += ConcatDataset([dataset])\n",
    "        self.dataset = _dataset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset.__getitem__(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "class CachingDataset:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "            \n",
    "    @lru_cache(maxsize=None)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset.__getitem__(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    \n",
    "class PreloadingDataset:\n",
    "    def __init__(self, dataset, num_proc=False, progress=None):\n",
    "        self.dataset = dataset\n",
    "        self.num_proc = num_proc\n",
    "        self.progress = progress\n",
    "        if self.num_proc:\n",
    "            self.data = utils.mp_func_gen(self.preload_data,\n",
    "                                             range(len(self.dataset)),\n",
    "                                             n=self.num_proc,\n",
    "                                             progress=progress)\n",
    "        else:\n",
    "            self.data = self.preload_data(range(len(self.dataset)))\n",
    "        \n",
    "    def preload_data(self, args):\n",
    "        idxs = args\n",
    "        data = []\n",
    "        if self.progress is not None and not self.num_proc: idxs = self.progress(idxs)\n",
    "        for i in idxs:\n",
    "            r = self.dataset.__getitem__(i)\n",
    "            data.append(r)\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "class GpuPreloadingDataset:\n",
    "    def __init__(self, dataset, devices):\n",
    "        self.dataset = dataset\n",
    "        self.devices = devices\n",
    "        self.data = self.preload_data()\n",
    "        \n",
    "    def preload_data(self):\n",
    "        data = []\n",
    "        for i in range(len(self.dataset)):\n",
    "            item, idx = self.dataset.__getitem__(i)\n",
    "            item = item.to(self.devices[0])\n",
    "            data.append((item, idx))\n",
    "        return data\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetCatalog():\n",
    "    DATA_DIR = \"/tmp/\"\n",
    "    DATA_DIR_MNT = \"/mnt/tmp\"\n",
    "    \n",
    "    DATASETS = {\n",
    "        \"default\": {\n",
    "            'factory':'default',\n",
    "            \"root\": \"def_root\",\n",
    "        }\n",
    "    }\n",
    "    @staticmethod \n",
    "    def create_factory_dict(data_dir, dataset_attrs):\n",
    "        #{factory:Dataset, args:args}\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @classmethod \n",
    "    def get(cls, name):\n",
    "        try:\n",
    "            attrs = cls.DATASETS[name]\n",
    "        except:\n",
    "            print(cls.DATASETS)\n",
    "            raise RuntimeError(\"Dataset not available: {}\".format(name))\n",
    "            \n",
    "        if os.path.exists(cls.DATA_DIR):\n",
    "            data_dir = cls.DATA_DIR\n",
    "        elif os.path.exists(cls.DATA_DIR_MNT):\n",
    "            data_dir = cls.DATA_DIR_MNT\n",
    "            \n",
    "        return cls.create_factory_dict(data_dir, attrs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_factories = {'termit':TermitDataset}\n",
    "# transform_factories = {'TRAIN':{'factory':TransformDataset_Partial_HARD, 'transform_getter':get_aug}}\n",
    "# extend_factories = {'GPU_PRELOAD':GpuPreloadingDataset_Partial_GPU0}\n",
    "# dataset_types = ['TRAIN', 'VALID', 'TEST']\n",
    "# datasets = {'TRAIN': dataset1, 'VALID': ...}\n",
    "\n",
    "def extend_dataset(ds, data_field, extend_factories):\n",
    "    for k, factory in extend_factories.items():\n",
    "        field_val = data_field.get(k, None) \n",
    "        if field_val:\n",
    "            args = {}\n",
    "            if isinstance(field_val, dict): args.update(field_val)\n",
    "            ds = factory(ds, **args)\n",
    "    return ds\n",
    "\n",
    "def create_extensions(cfg, datasets, extend_factories):\n",
    "    extended_datasets = {}\n",
    "    for kind, ds in datasets.items():\n",
    "        extended_datasets[kind] = extend_dataset(ds, cfg.DATA[kind], extend_factories)\n",
    "    return extended_datasets\n",
    "\n",
    "\n",
    "def _create_dataset_fact(catalog, ds, dataset_factories):\n",
    "    dataset_attrs = catalog.get(ds)\n",
    "    factory = dataset_factories[dataset_attrs['factory']]\n",
    "    return factory(**dataset_attrs['args'])\n",
    "\n",
    "def create_datasets(cfg,\n",
    "                    all_datasets,\n",
    "                    dataset_types=['TRAIN', 'VALID', 'TEST']):\n",
    "\n",
    "    converted_datasets = {}\n",
    "    for dataset_type in dataset_types:\n",
    "        data_field = cfg.DATA[dataset_type]\n",
    "        datasets_strings = data_field.DATASETS\n",
    "        \n",
    "        if datasets_strings:\n",
    "            datasets = [all_datasets[ds] for ds in datasets_strings]\n",
    "            ds = ConcatDataset(datasets) if len(datasets)>1 else datasets[0] \n",
    "            converted_datasets[dataset_type] = ds\n",
    "    return converted_datasets\n",
    "\n",
    "# def __create_datasets(cfg,\n",
    "#                    catalog,\n",
    "#                    dataset_factories,\n",
    "#                    dataset_types=['TRAIN', 'VALID', 'TEST']):\n",
    "\n",
    "#     converted_datasets = {}\n",
    "#     for dataset_type in dataset_types:\n",
    "#         data_field = cfg.DATA[dataset_type]\n",
    "#         datasets_strings = data_field.DATASETS\n",
    "\n",
    "#         if datasets_strings:\n",
    "#             datasets = [_create_dataset_fact(catalog, ds, dataset_factories) for ds in datasets_strings]\n",
    "#             ds = ConcatDataset(datasets) if len(datasets)>1 else datasets[0] \n",
    "#             converted_datasets[dataset_type] = ds\n",
    "#     return converted_datasets\n",
    "\n",
    "def create_transforms(cfg,\n",
    "                      transform_factories,\n",
    "                      dataset_types=['TRAIN', 'VALID', 'TEST']):\n",
    "    transformers = {}\n",
    "    for dataset_type in dataset_types:\n",
    "        aug_type = cfg.TRANSFORMERS[dataset_type]['AUG']\n",
    "        args={\n",
    "            'aug_type':aug_type,\n",
    "            'transforms_cfg':cfg.TRANSFORMERS\n",
    "        }\n",
    "        if transform_factories[dataset_type]['factory'] is not None:\n",
    "            transform_getter = transform_factories[dataset_type]['transform_getter'](**args)\n",
    "            transformer = partial(transform_factories[dataset_type]['factory'], transforms=transform_getter)\n",
    "        else:\n",
    "            transformer = lambda x: x\n",
    "        transformers[dataset_type] = transformer\n",
    "    return transformers    \n",
    "\n",
    "def apply_transforms_datasets(datasets, transforms):\n",
    "    return {dataset_type:transforms[dataset_type](dataset) for dataset_type, dataset in datasets.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloaders(datasets, samplers=None, batch_sizes=None, num_workers=1, drop_last=False, pin=False):\n",
    "    dls = {}\n",
    "    for kind, dataset in datasets.items():\n",
    "        sampler = samplers[kind]    \n",
    "        shuffle = kind == 'TRAIN' if sampler is None else False\n",
    "        batch_size = batch_sizes[kind] if batch_sizes[kind] is not None else 1\n",
    "        dls[kind] = create_dataloader(dataset, sampler, shuffle, batch_size, num_workers, drop_last, pin)\n",
    "            \n",
    "    return dls\n",
    "    \n",
    "def create_dataloader(dataset, sampler=None, shuffle=False, batch_size=1, num_workers=1, drop_last=False, pin=False):\n",
    "    collate_fn=None\n",
    "    assert not(sampler is not None and shuffle)\n",
    "    \n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=collate_fn,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "imgs_path = './test_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "d1 = Dataset(imgs_path, 'aimg*.png')\n",
    "assert len(d1) == 1\n",
    "try:\n",
    "    d1[0]\n",
    "except NotImplementedError:\n",
    "    pass\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "d2 = ImageDataset(imgs_path, 'aimg_*')\n",
    "d2.process_item = lambda x: np.array(x)\n",
    "assert len(d2) == 1\n",
    "d3 = ImageDataset(imgs_path, 'mask_*')\n",
    "d3.process_item = lambda x: np.array(x)\n",
    "assert len(d3) == 1\n",
    "d2[0].shape, d3[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "d4 = PairDataset(d2, d3)\n",
    "assert len(d4) == 1\n",
    "i,ii = d4[0]\n",
    "j, jj = d2[0], d3[0]\n",
    "np.allclose(i,j), np.allclose(ii,jj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transforms dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "transforms = albu.Compose([albu.CenterCrop(50, 50)])\n",
    "d5 = TransformDataset(d4, transforms=transforms, is_masked=True)\n",
    "i = d5[0]\n",
    "i[0].shape, i[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "mult = 2\n",
    "d6 = MultiplyDataset(d2, mult)\n",
    "assert len(d6) // mult == len(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "d7 = CachingDataset(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 100\n",
    "d7[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%timeit -r 1 -n 5\n",
    "d2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "_d8 = ImageDataset(imgs_path, 'aimg_9*.png')\n",
    "d8 = PreloadingDataset(_d8, num_proc=8)\n",
    "assert len(_d8) == len(d8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "%%timeit -r 10 -n 100\n",
    "d8[18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "class MyDatasetCatalog(DatasetCatalog):\n",
    "    DATA_DIR = \"test_data/\"\n",
    "    DATA_DIR_MNT = \"/mnt/input/term\"\n",
    "    \n",
    "    DATASETS = {\n",
    "        \"test_data\": {\n",
    "                        'factory':'factory_test',\n",
    "                        'path_args':{\n",
    "                                        \"root\": \"validation_1_1\",\n",
    "                                    },\n",
    "                        'kwargs':{\n",
    "                                        \"pattern\": 'aimg*.png'\n",
    "                                }\n",
    "            \n",
    "        },\n",
    "        \"test_data_masks\": {\n",
    "            'factory':'factory_test_masks',\n",
    "            'path_args':{\n",
    "                    \"root\": \"validation_1_1\",\n",
    "            },\n",
    "            'kwargs':{\n",
    "                    \"pattern\": 'aimg*.png'\n",
    "            }\n",
    "        },\n",
    "        \"test_data_joined\": {\n",
    "            'factory':'factory_test_joined',\n",
    "            'path_args':{\n",
    "                    \"root1\": \"validation_1_1\",\n",
    "                    \"root2\": \"validation_1_1\",\n",
    "            },\n",
    "            'kwargs':{\n",
    "                    \"pattern1\": 'aimg*.png',\n",
    "                    \"pattern2\": 'mask*.png'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get(name): return super(MyDatasetCatalog, MyDatasetCatalog).get(name)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_factory_dict(data_dir, dataset_attrs):\n",
    "        factory = dataset_attrs['factory']\n",
    "        allowed_facts = [v['factory']  for v in MyDatasetCatalog.DATASETS.values()]\n",
    "        if factory not in allowed_facts: raise RuntimeError(f' Uknnown factory type: {factory}' )\n",
    "        \n",
    "        path_args = {k:os.path.join(data_dir, v) for k, v in dataset_attrs['path_args'].items()}\n",
    "        return dict(factory=factory, args={**path_args, **dataset_attrs['kwargs']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "test_fact_args = MyDatasetCatalog.get(name='test_data_masks')\n",
    "test_fact_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "test_fact_args = MyDatasetCatalog.get(name='test_data_joined')\n",
    "test_fact_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "from nb_configer import cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "yaml_str = '''\n",
    "    DATA:\n",
    "      TRAIN:\n",
    "        DATASETS: ['test_data_joined', 'test_data_joined']\n",
    "        GPU_PRELOAD: False\n",
    "        PRELOAD: True\n",
    "        CACHE: False\n",
    "      VALID:\n",
    "        DATASETS: ['test_data']\n",
    "      TEST:\n",
    "        DATASETS: ['test_data']\n",
    "\n",
    "    TRANSFORMERS:\n",
    "      TRAIN:\n",
    "        AUG: 'test'\n",
    "      VALID:\n",
    "        AUG: 'val'\n",
    "      TEST:\n",
    "        AUG: 'test'\n",
    "\n",
    "      CROP: [256, 256]\n",
    "      RESIZE: [512, 512]\n",
    "\n",
    "    TRAIN:\n",
    "      NUM_WORKERS: 0\n",
    "      BATCH_SIZE: 128\n",
    "\n",
    "    VALID:\n",
    "      NUM_WORKERS: 4\n",
    "      BATCH_SIZE: 1\n",
    "    '''\n",
    "yd = yaml.safe_load(yaml_str)\n",
    "with open('/tmp/t.yaml', 'w') as f:\n",
    "    yaml.safe_dump(yd, f)\n",
    "cfg.merge_from_file('/tmp/t.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "pprint(cfg.DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "def test_trans_get(aug_type, transforms_cfg):\n",
    "    w,h = transforms_cfg['RESIZE']\n",
    "    return albu.Compose([albu.CenterCrop(h, w)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "class ImageDatasetArray(ImageDataset):\n",
    "    def process_item(self, item): return np.array(item)\n",
    "class PairImageDataset(PairDataset):\n",
    "    def __init__(self, root1, pattern1, root2, pattern2):\n",
    "        self.ds1 = ImageDatasetArray(root1, pattern1)\n",
    "        self.ds2 = ImageDatasetArray(root2, pattern2)\n",
    "        assert len(self.ds1) == len(self.ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_factories = {'factory_test': ImageDatasetArray, 'factory_test_joined': PairImageDataset}\n",
    "transform_factory = {\n",
    "    'TRAIN':{'factory':partial(TransformDataset, is_masked=True), 'transform_getter':test_trans_get},\n",
    "    'TEST':{'factory':TransformDataset, 'transform_getter':test_trans_get},\n",
    "    'VALID':{'factory':TransformDataset, 'transform_getter':test_trans_get},\n",
    "}\n",
    "extend_factories = {\n",
    "    'GPU_PRELOAD':GpuPreloadingDataset,\n",
    "    'PRELOAD':partial(PreloadingDataset, num_proc=8),\n",
    "    'CACHE':CachingDataset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "active-ipynb"
    ]
   },
   "outputs": [],
   "source": [
    "datasets = create_datasets(cfg, MyDatasetCatalog, dataset_factories)\n",
    "datasets = create_extensions(cfg, datasets, extend_factories)\n",
    "\n",
    "transforms = create_transforms(cfg, transform_factory)\n",
    "datasets = apply_transforms_datasets(datasets, transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "notebooks//ipynb,shallow//py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

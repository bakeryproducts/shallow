
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/callbacks.ipynb

import time
from functools import partial

import torch
from torch.utils.tensorboard import SummaryWriter

from shallow import utils, meters
#from utils import GetAttr

class CancelFitException(Exception): pass

class Callback(utils.GetAttr):
    _default='learner'
    def __init__(self, *args, **kwargs):
        utils.store_attr(self, locals())

class ParamSchedulerCB(Callback):
    def __init__(self, phase, pname, sched_func):
        self.pname, self.sched_func = pname, sched_func
        setattr(self, phase, self.set_param)

    def set_param(self):
        setattr(self.learner, self.pname, self.sched_func(self.np_epoch))

class SetupLearnerCB(Callback):
    def before_batch(self):
        xb,yb = to_device(self.batch)
        self.learner.batch = tfm_x(xb),yb

    def before_fit(self): self.model.cuda()

class TrackResultsCB(Callback):
    def before_epoch(self): self.accs,self.losses,self.ns = [],[],[]

    def after_epoch(self):
        n = sum(self.ns)
        print(self.n_epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n)

    def after_batch(self):
        xb, yb = self.batch
        acc = (self.preds.argmax(dim=1)==yb).float().sum()
        self.accs.append(acc)
        n = len(xb)
        self.losses.append(self.loss*n)
        self.ns.append(n)

class LRFinderCB(Callback):
    def before_fit(self):
        self.losses,self.lrs = [],[]
        self.learner.lr = 1e-6

    def before_batch(self):
        if not self.model.training: return
        self.learner.lr *= 1.2
        print(self.lr)

    def after_batch(self):
        if not self.model.training: return
        if self.lr > 1 or torch.isnan(self.loss): raise CancelFitException
        self.losses.append(self.loss.item())
        self.lrs.append(self.lr)


class TimerCB(Callback):
    def __init__(self, Timer=None, logger=None):
        self.logger = logger
        self.perc=90
        if Timer is None: Timer = meters.StopwatchMeter
        self.batch_timer = Timer()
        self.epoch_timer = Timer()

    def before_batch(self): self.batch_timer.start()
    def before_epoch(self): self.epoch_timer.start()
    def after_batch(self):  self.batch_timer.stop()
    def log(self, m): self.logger.info(m) if self.logger is not None else False

    def after_epoch(self):
        self.epoch_timer.stop()
        bs, es = self.learner.dl.batch_size, len(self.learner.dl)
        self.log(f'\tEpoch {self.n_epoch}: {self.epoch_timer.last: .3f} s,'+
                 f'{bs * es/self.epoch_timer.last: .3f} im/s; '+
                 f'batch {self.batch_timer.avg: .3f} s'   )
        self.batch_timer.reset()

    def after_fit(self):
        et = self.epoch_timer
        em = et.avg
        estd = ((et.p(self.perc) - em) + (em - et.p(1-self.perc))) / 2
        self.log(f'\tEpoch average time: {em: .3f} +- {estd: .3f} s')


class CheckpointCB(Callback):
    def __init__(self, save_path, save_step=None):
        utils.store_attr(self, locals())
        self.pct_counter = None if isinstance(self.save_step, int) else self.save_step

    def after_epoch(self):
        save = False
        if self.n_epoch == self.total_epochs - 1: save=True
        elif isinstance(self.save_step, int): save = self.save_step % self.n_epoch == 0
        else:
            if self.np_epoch > self.pct_counter:
                save = True
                self.pct_counter += self.save_step

        if save:
            torch.save({
                            'epoch': self.n_epoch,
                            'loss': self.loss,
                            'model_state':self.model.state_dict(),
                            'opt_state':self.opt.state_dict(),
                        }, str(self.save_path / f'e{self.n_epoch}_t{self.total_epochs}_1e4l{int(1e4*self.loss)}.pth'))


class HooksCB(Callback):
    def __init__(self, hook_func, hookable_layers, perc_start=.5, logger=None):
        utils.store_attr(self, locals())
        self.hooks = Hooks(self.hookable_layers, self.hook_func)
        self.do_once = True

    def before_batch(self):
        if self.do_once and self.np_batch > self.perc_start:
            self.log(f'Gathering activations at batch {self.np_batch}')
            self.do_once = False
            self.hooks.attach()

    def after_batch(self): self.hooks.detach()
    def after_epoch(self): self.do_once = True
    def log(self, m): self.logger.debug(m) if self.logger is not None else False

class Hook():
    def __init__(self, m, f): self.m, self.f = m, f
    def attach(self): self.hook = self.m.register_forward_hook(partial(self.f, self))
    def detach(self):
        if hasattr(self, 'hook') :self.hook.remove()
    def __del__(self): self.detach()

class Hooks(utils.ListContainer):
    def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])
    def __enter__(self, *args):
        self.attach()
        return self
    def __exit__ (self, *args): self.detach()
    def __del__(self): self.detach()

    def __delitem__(self, i):
        self[i].detach()
        super().__delitem__(i)

    def attach(self):
        for h in self: h.attach()

    def detach(self):
        for h in self: h.detach()

def get_hookable(model, conv=False, convtrans=False, lrelu=False, relu=False, bn=False, verbose=False):
    hookable = []
    for m in model.modules():
        if isinstance(m, torch.nn.Conv2d):
            if conv: hookable.append(m)
        if isinstance(m, torch.nn.ConvTranspose2d):
            if convtrans: hookable.append(m)
        elif isinstance(m, torch.nn.LeakyReLU):
            if lrelu: hookable.append(m)
        elif isinstance(m, torch.nn.ReLU):
            if relu: hookable.append(m)
        elif isinstance(m, torch.nn.BatchNorm2d):
            if bn: hookable.append(m)
        else:
            if verbose: print(m)
    return hookable

def append_stats(hook, mod, inp, outp, bins=100, vmin=0, vmax=0):
    if not hasattr(hook,'stats'): hook.stats = ([],[],[])
    means,stds,hists = hook.stats
    means.append(outp.data.mean().cpu())
    stds .append(outp.data.std().cpu())
    hists.append(outp.data.cpu().histc(bins,vmin,vmax))

def append_stats_buffered(hook, mod, inp, outp, device=torch.device('cpu'), bins=100, vmin=0, vmax=0):
    if not hasattr(hook,'stats'): hook.stats = (utils.TorchBuffer(shape=(1,), device=device),
                                                utils.TorchBuffer(shape=(1,), device=device),
                                                utils.TorchBuffer(shape=(bins,), device=device)
                                               )
    means,stds,hists = hook.stats
    means.push(outp.data.mean())
    stds .push(outp.data.std())
    hists.push(outp.data.float().histc(bins,vmin,vmax))
